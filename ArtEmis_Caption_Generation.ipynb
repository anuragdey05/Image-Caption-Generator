{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Training Pipeline for ArtEmis Image Captioning\n",
        "\n",
        "This notebook trains and evaluates both CNN+LSTM and Vision-Language Transformer models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from src.datasets.artemis_dataset import ArtemisDataset, collate_captions\n",
        "from src.models.cnn_lstm.cnn_lstm_model import ImageCaptioningCNNLSTM\n",
        "from src.models.vit.caption_transformer import CaptionTransformer\n",
        "from src.models.vit.config_transformer import TransformerHyperParams\n",
        "from src.utils.tokenization import Tokenizer\n",
        "from src.evaluation.evaluate_models import evaluate_cnn_lstm, evaluate_transformer\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('logs/training.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Data paths\n",
        "    \"csv_path\": project_root / \"data\" / \"artemis_sample_8k.csv\",\n",
        "    \"images_root\": project_root / \"data\" / \"wikiart_sample_128\",\n",
        "    \"vocab_path\": project_root / \"src\" / \"utils\" / \"vocab.json\",\n",
        "    \"split_path\": project_root / \"data\" / \"splits.json\",\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    \"batch_size\": 32,\n",
        "    \"num_workers\": 4,\n",
        "    \"max_len\": 32,\n",
        "    \"epochs_cnn_lstm\": 15,\n",
        "    \"epochs_transformer\": 15,\n",
        "    \"lr_cnn_lstm\": 1e-3,\n",
        "    \"lr_transformer\": 1e-4,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"max_grad_norm\": 5.0,\n",
        "    \"early_stop_patience\": 5,\n",
        "    \n",
        "    # Model configs\n",
        "    \"cnn_lstm\": {\n",
        "        \"embedding_dim\": 256,\n",
        "        \"hidden_dim\": 256,\n",
        "        \"num_layers\": 1,\n",
        "        \"dropout\": 0.1,\n",
        "        \"image_feat_dim\": 256,\n",
        "    },\n",
        "    \"transformer\": {\n",
        "        \"d_model\": 256,\n",
        "        \"num_heads\": 8,\n",
        "        \"num_layers\": 4,\n",
        "        \"patch_size\": 16,\n",
        "        \"dropout\": 0.1,\n",
        "        \"mlp_ratio\": 4.0,\n",
        "    },\n",
        "    \n",
        "    # Output paths\n",
        "    \"checkpoint_dir_cnn_lstm\": project_root / \"checkpoints\" / \"cnn_lstm\",\n",
        "    \"checkpoint_dir_transformer\": project_root / \"checkpoints\" / \"transformer\",\n",
        "    \"results_dir\": project_root / \"results\",\n",
        "    \n",
        "    # Device\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "}\n",
        "\n",
        "# Create directories\n",
        "for path in [CONFIG[\"checkpoint_dir_cnn_lstm\"], CONFIG[\"checkpoint_dir_transformer\"], CONFIG[\"results_dir\"], project_root / \"logs\"]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Using device: {CONFIG['device']}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Tokenizer and Build Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = Tokenizer.load(CONFIG[\"vocab_path\"])\n",
        "print(f\"Vocabulary size: {len(tokenizer.word2idx)}\")\n",
        "print(f\"Special tokens: PAD={tokenizer.pad_idx}, BOS={tokenizer.bos_idx}, EOS={tokenizer.eos_idx}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build train/val/test splits\n",
        "def build_splits(csv_path, split_path, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
        "    if split_path.exists():\n",
        "        with open(split_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "    \n",
        "    df = pd.read_csv(csv_path)\n",
        "    unique_images = df[\"painting\"].dropna().astype(str).unique().tolist()\n",
        "    \n",
        "    rng = random.Random(seed)\n",
        "    rng.shuffle(unique_images)\n",
        "    \n",
        "    total = len(unique_images)\n",
        "    test_count = max(1, int(total * test_ratio))\n",
        "    val_count = max(1, int(total * val_ratio))\n",
        "    train_count = total - val_count - test_count\n",
        "    \n",
        "    splits = {\n",
        "        \"train\": unique_images[:train_count],\n",
        "        \"val\": unique_images[train_count:train_count + val_count],\n",
        "        \"test\": unique_images[train_count + val_count:],\n",
        "    }\n",
        "    \n",
        "    split_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(split_path, 'w') as f:\n",
        "        json.dump(splits, f, indent=2)\n",
        "    \n",
        "    return splits\n",
        "\n",
        "splits = build_splits(CONFIG[\"csv_path\"], CONFIG[\"split_path\"], val_ratio=0.1, test_ratio=0.1)\n",
        "print(f\"Train: {len(splits['train'])}, Val: {len(splits['val'])}, Test: {len(splits['test'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = ArtemisDataset(\n",
        "    csv_path=CONFIG[\"csv_path\"],\n",
        "    img_root=CONFIG[\"images_root\"],\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=CONFIG[\"max_len\"],\n",
        "    image_filter=set(splits[\"train\"]),\n",
        ")\n",
        "\n",
        "val_dataset = ArtemisDataset(\n",
        "    csv_path=CONFIG[\"csv_path\"],\n",
        "    img_root=CONFIG[\"images_root\"],\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=CONFIG[\"max_len\"],\n",
        "    image_filter=set(splits[\"val\"]),\n",
        ")\n",
        "\n",
        "test_dataset = ArtemisDataset(\n",
        "    csv_path=CONFIG[\"csv_path\"],\n",
        "    img_root=CONFIG[\"images_root\"],\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=CONFIG[\"max_len\"],\n",
        "    image_filter=set(splits[\"test\"]),\n",
        ")\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Val dataset: {len(val_dataset)} samples\")\n",
        "print(f\"Test dataset: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loaders with multiprocessing\n",
        "device = torch.device(CONFIG[\"device\"])\n",
        "pin_memory = device.type == \"cuda\"\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    collate_fn=collate_captions,\n",
        "    pin_memory=pin_memory,\n",
        "    persistent_workers=CONFIG[\"num_workers\"] > 0,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    collate_fn=collate_captions,\n",
        "    pin_memory=pin_memory,\n",
        "    persistent_workers=CONFIG[\"num_workers\"] > 0,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    collate_fn=collate_captions,\n",
        "    pin_memory=pin_memory,\n",
        "    persistent_workers=CONFIG[\"num_workers\"] > 0,\n",
        ")\n",
        "\n",
        "print(f\"Data loaders created with num_workers={CONFIG['num_workers']}, pin_memory={pin_memory}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train CNN+LSTM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize CNN+LSTM model\n",
        "cnn_lstm_model = ImageCaptioningCNNLSTM(\n",
        "    vocab_size=len(tokenizer.word2idx),\n",
        "    **CONFIG[\"cnn_lstm\"],\n",
        ").to(device)\n",
        "\n",
        "print(f\"CNN+LSTM model parameters: {sum(p.numel() for p in cnn_lstm_model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training functions for CNN+LSTM\n",
        "def train_epoch_cnn_lstm(model, loader, optimizer, criterion, device, scaler, max_grad_norm):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images = batch[\"images\"].to(device, non_blocking=True)\n",
        "        captions_in = batch[\"captions_in\"].to(device, non_blocking=True)\n",
        "        captions_out = batch[\"captions_out\"].to(device, non_blocking=True)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(enabled=scaler.is_enabled()):\n",
        "            logits = model(images, captions_in)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), captions_out.view(-1))\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch_cnn_lstm(model, loader, criterion, device, use_amp):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = batch[\"images\"].to(device, non_blocking=True)\n",
        "        captions_in = batch[\"captions_in\"].to(device, non_blocking=True)\n",
        "        captions_out = batch[\"captions_out\"].to(device, non_blocking=True)\n",
        "        \n",
        "        with autocast(enabled=use_amp):\n",
        "            logits = model(images, captions_in)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), captions_out.view(-1))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CNN+LSTM\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_idx)\n",
        "optimizer = Adam(cnn_lstm_model.parameters(), lr=CONFIG[\"lr_cnn_lstm\"])\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "use_amp = device.type == \"cuda\"\n",
        "scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "cnn_lstm_history = {\"train_loss\": [], \"val_loss\": []}\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "for epoch in range(1, CONFIG[\"epochs_cnn_lstm\"] + 1):\n",
        "    train_loss = train_epoch_cnn_lstm(cnn_lstm_model, train_loader, optimizer, criterion, device, scaler, CONFIG[\"max_grad_norm\"])\n",
        "    val_loss = eval_epoch_cnn_lstm(cnn_lstm_model, val_loader, criterion, device, use_amp)\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    cnn_lstm_history[\"train_loss\"].append(train_loss)\n",
        "    cnn_lstm_history[\"val_loss\"].append(val_loss)\n",
        "    \n",
        "    logger.info(f\"CNN+LSTM Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stop_counter = 0\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            \"model_state\": cnn_lstm_model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"config\": CONFIG[\"cnn_lstm\"],\n",
        "        }\n",
        "        torch.save(checkpoint, CONFIG[\"checkpoint_dir_cnn_lstm\"] / f\"best_epoch{epoch:03d}_val{val_loss:.3f}.pt\")\n",
        "        print(f\"Saved checkpoint: best_epoch{epoch:03d}_val{val_loss:.3f}.pt\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= CONFIG[\"early_stop_patience\"]:\n",
        "            logger.info(\"Early stopping triggered\")\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Transformer Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Transformer model\n",
        "hyper = TransformerHyperParams(\n",
        "    vocab_size=len(tokenizer.word2idx),\n",
        "    pad_idx=tokenizer.pad_idx,\n",
        "    max_seq_len=CONFIG[\"max_len\"],\n",
        "    **CONFIG[\"transformer\"],\n",
        ")\n",
        "\n",
        "transformer_model = CaptionTransformer(\n",
        "    vision_config=hyper.vision_config(),\n",
        "    decoder_config=hyper.decoder_config(),\n",
        ").to(device)\n",
        "\n",
        "print(f\"Transformer model parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training functions for Transformer\n",
        "def train_epoch_transformer(model, loader, optimizer, criterion, device, scaler, max_grad_norm):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images = batch[\"images\"].to(device, non_blocking=True)\n",
        "        captions_in = batch[\"captions_in\"].to(device, non_blocking=True)\n",
        "        captions_out = batch[\"captions_out\"].to(device, non_blocking=True)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(enabled=scaler.is_enabled()):\n",
        "            logits = model(images, captions_in)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), captions_out.view(-1))\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch_transformer(model, loader, criterion, device, use_amp):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = batch[\"images\"].to(device, non_blocking=True)\n",
        "        captions_in = batch[\"captions_in\"].to(device, non_blocking=True)\n",
        "        captions_out = batch[\"captions_out\"].to(device, non_blocking=True)\n",
        "        \n",
        "        with autocast(enabled=use_amp):\n",
        "            logits = model(images, captions_in)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), captions_out.view(-1))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Transformer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_idx)\n",
        "optimizer = AdamW(transformer_model.parameters(), lr=CONFIG[\"lr_transformer\"], weight_decay=CONFIG[\"weight_decay\"])\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "use_amp = device.type == \"cuda\"\n",
        "scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "transformer_history = {\"train_loss\": [], \"val_loss\": []}\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "for epoch in range(1, CONFIG[\"epochs_transformer\"] + 1):\n",
        "    train_loss = train_epoch_transformer(transformer_model, train_loader, optimizer, criterion, device, scaler, CONFIG[\"max_grad_norm\"])\n",
        "    val_loss = eval_epoch_transformer(transformer_model, val_loader, criterion, device, use_amp)\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    transformer_history[\"train_loss\"].append(train_loss)\n",
        "    transformer_history[\"val_loss\"].append(val_loss)\n",
        "    \n",
        "    logger.info(f\"Transformer Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stop_counter = 0\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            \"model_state\": transformer_model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"hyperparams\": hyper.__dict__,\n",
        "        }\n",
        "        torch.save(checkpoint, CONFIG[\"checkpoint_dir_transformer\"] / f\"best_epoch{epoch:03d}_val{val_loss:.3f}.pt\")\n",
        "        print(f\"Saved checkpoint: best_epoch{epoch:03d}_val{val_loss:.3f}.pt\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= CONFIG[\"early_stop_patience\"]:\n",
        "            logger.info(\"Early stopping triggered\")\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Plot Training Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# CNN+LSTM\n",
        "axes[0].plot(cnn_lstm_history[\"train_loss\"], label=\"Train Loss\", marker='o')\n",
        "axes[0].plot(cnn_lstm_history[\"val_loss\"], label=\"Val Loss\", marker='s')\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"CNN+LSTM Training Curves\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Transformer\n",
        "axes[1].plot(transformer_history[\"train_loss\"], label=\"Train Loss\", marker='o')\n",
        "axes[1].plot(transformer_history[\"val_loss\"], label=\"Val Loss\", marker='s')\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Loss\")\n",
        "axes[1].set_title(\"Transformer Training Curves\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(CONFIG[\"results_dir\"] / \"training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Models on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate CNN+LSTM\n",
        "print(\"Evaluating CNN+LSTM model...\")\n",
        "cnn_lstm_scores = evaluate_cnn_lstm(\n",
        "    cnn_lstm_model,\n",
        "    tokenizer,\n",
        "    test_loader,\n",
        "    device,\n",
        "    CONFIG[\"max_len\"],\n",
        ")\n",
        "\n",
        "print(\"\\nCNN+LSTM Results:\")\n",
        "for metric, score in cnn_lstm_scores.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Transformer\n",
        "print(\"Evaluating Transformer model...\")\n",
        "transformer_scores = evaluate_transformer(\n",
        "    transformer_model,\n",
        "    tokenizer,\n",
        "    test_loader,\n",
        "    device,\n",
        "    CONFIG[\"max_len\"],\n",
        ")\n",
        "\n",
        "print(\"\\nTransformer Results:\")\n",
        "for metric, score in transformer_scores.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation results\n",
        "results = {\n",
        "    \"cnn_lstm\": cnn_lstm_scores,\n",
        "    \"transformer\": transformer_scores,\n",
        "    \"config\": CONFIG,\n",
        "    \"training_history\": {\n",
        "        \"cnn_lstm\": cnn_lstm_history,\n",
        "        \"transformer\": transformer_history,\n",
        "    },\n",
        "}\n",
        "\n",
        "with open(CONFIG[\"results_dir\"] / \"evaluation_results.json\", 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nResults saved to {CONFIG['results_dir'] / 'evaluation_results.json'}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.9.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
